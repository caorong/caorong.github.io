---
layout: post
title: "断点续传"
description: ""
category: "crawler"
tags: [java,download,crawler,web]
---

额，距离上次写又过了一个月了，实在想不出写什么，就拿这两天写的代码凑凑数吧～。

最近爬虫在下载cntv，和sohu的视频时，发现错误率非常高（socket time out），一开始以为是联通和电信的关系。因为下载机器是联通的网，而cdn解析出来的ip是电信的ip，所以造成的速度慢。不过，我发现即使我在家里（电信），在chrome下视频依然抽搐不堪，Network 里面一排鲜艳的500。

但是抽搐并不代表不能下载，如果能接像浏览器一样接着上次下载的地方继续下就好了。

于是，google了下断点续传原理，发现其实没我想的那么难。

当然，首先需要服务器支持。有时用迅雷下载时，会提示该任务不支持断点续传，那么说明这个任务所在的server不支持。

她的原理是在http header里面加一个Range，两边数字分别表示startPosition和endPosition，如果不填写endPosition，默认表示结尾

```html
Range: bytes=0-
Range: bytes=0-999
```

还有个要注意的是，如果指定了Range，那么server返回的就不再是200了，而是206

以上都知道了后要写个断点续传就非常简单了，只要cache`IOException`后，记录下当前接收到的字节数，下次请求时设置下Range即可。

而且，多线程下载也是这个原理。第一次请求获取他的contentlength后，然后释放该链接，重新create n个线程，分别指定他们的Range，然后就慢慢等他下完吧。


#### code reference
[爬虫断点续传](https://github.com/caorong/crawler-analyzer/blob/master/src/main/java/com/ximalaya/crawler/analysis/utils/AbstractFetcher.java#L243)

[多线程下载](https://github.com/daimajia/java-multithread-downloader)